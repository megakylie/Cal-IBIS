---
title: "Remove_Duplicate_Data"
author: "Josie Lesage"
date: "12/9/2020"
output: html_document
editor: "Kylie Etter"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = TRUE)

# Setup packages
library(rgbif)
library(tidyverse)
library(stringr)
```

*KJE Step 2A. First you are going to need to download some of the non-GBIF occurrences you just uploaded to Cal-IBIS. You can search for duplicates in Cal-IBIS within a collection (i.e. Bugs-Ecdysis), but you don't have that function to compare between collections (i.e. Bugs-Ecdysis and Bugs-GBIF), so this step is important to avoid inflated numbers caused by duplication of the same occurrence. Download the Bugs (2), Fungi (1) and Plants (2) collections and put them in your respective "Data/MonthYear/" folder with the following names:

Bugs-Ecdysis & Scan Portal : Bugs_EcdysisSCAN_Clean.csv
Fungi - Mycology Portal: MyCoPortal_Clean.csv
Plants- Consortium of California Herbaria 2 & Plants - SEINet: CCH2SEINet_Clean.csv

To download data from cal-ibis click "Search Collections", click on the collection(s) you want to download, click "Next" on the "Search Criteria" page because we want everything, change the DownloadType to "CSV/ZIP", and then finally click the download button. Keep everything as is and download the data as a zip file, rename the occurrences.csv as instructed above and place it in the folder (only the renamed occurences.csv should go in the folder).


------------------------------------------------------------------------
Step 2B. 

## Cleaning data and removing potential duplicates

The code chunks below will remove the duplicated occurrence records for all records gathered from GBIF that are already present in the datasets downloaded from other sources (fungi, arthros, plants). To do this, we will directly call on the other dataset data and remove all duplicates from the data we downloaded from GBIF above.

Below that, there's a chunk to just extract and save the .csv files to the clean data folder.

To begin, use ctrl+F to find and *replace all* the previous date ("January2024") with the new date, the name of the MonthYear folder you just created.

All zip files should be extracted before running the code below. 

### Arthropods

```{r removing duplicates from Bugs}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU need the data changed with replace all instructed above__**
arthro_raw <- read.delim("Data/January2024/Raw/Arthropods/occurrence.txt") 

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# arthro_clean1 <- anti_join(arthro_raw, inCode, by = "institutionCode")
# arthro_clean2 <- anti_join(arthro_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
ecdysisscan_clean <- read_csv("Data/January2024/EcdysisSCAN_Clean.csv", 
                       col_types = cols(.default = "c")) 

# these steps clean the GBIF arthro data by filtering out anything that already has a match in Ecdysis or SCAN
arthro_process <- anti_join(arthro_raw, ecdysisscan_clean, by = "occurrenceID")



# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (arthro_process, 
           file = "Data/January2024//GBIFClean Data/GBIF_arthropodsBUGS_clean_January2024.csv",
           na="", row.names=FALSE)


#removing to make some more space, if you need any of these files agin you will have to redownload/clean
remove(arthro_raw, arthro_process, ecdysisscan_clean)

```

### Fungi

```{r removing duplicates from Fungi}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
fungi_raw <- read.delim("Data/January2024/Raw/Fungi/occurrence.txt")

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# fungi_clean1 <- anti_join(fungi_raw, inCode, by = "institutionCode")
# fungi_clean2 <- anti_join(fungi_clean1, inCode, by = "collectionCode")

## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
MyCo_clean <- read_csv("Data/January2024/MyCoPortal_Clean.csv", 
                       col_types = cols(.default = "c"))

# these steps clean the GBIF fungi data by filtering out anything that already has a match  in MyCoPortal
fungi_process <- anti_join(fungi_raw, MyCo_clean, by = "occurrenceID")


# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (fungi_process, 
           file = "Data/January2024/GBIFClean Data/GBIF_fungi_clean_January2024.csv",
           na="", row.names=FALSE)

#removing to make some more space
remove(fungi_raw, fungi_process, MyCo_clean)
```

### Plants

```{r removing duplicates from Plants}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
plant_raw <- read_tsv("Data/January2024/Raw/Plants/occurrence.txt", col_types = cols(.default = "c"))



## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# plant_clean1 <- anti_join(plant_raw, inCode, by = "institutionCode")
# plant_clean2 <- anti_join(plant_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
cch2seinet_clean <- read_csv("Data/January2024/CCH2SEINet_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF plant data by filtering out anything that already has a match in CCH or SEINet.
plant_process <- anti_join(plant_raw, cch2seinet_clean, by = "occurrenceID")


# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (plant_process, file = "Data/January2024/GBIFClean Data/GBIF_plants_clean_January2024.csv", na="", row.names=FALSE)

#removing to make some more space
remove(plant_raw, plant_process, cch2seinet_clean)
```

## Cut up the big files into multiple parts

Cutting the large datasets (>150000 obs.) into smaller pieces makes it easier to upload to symbiota. **In the future, we may need to increase the number of slices.**


```{r cut up big files}
amphibia <- read.delim("Data/January2024/Raw/Amphibia/occurrence.txt", quote = "")
mammalia <- read.delim("Data/January2024/Raw/Mammals/occurrence.txt", quote = "")
reptilia <- read.delim("Data/January2024/Raw/Reptiles/occurrence.txt", quote = "")
aves <- read.delim("Data/January2024/Raw/Aves/occurrence.txt", quote = "")
fish <- read.delim("Data/January2024/Raw/Fish/occurrence.txt", quote = "")
inverts <- read.delim("Data/January2024/Raw/Inverts/occurrence.txt", quote = "")

# There are just over 450,000 bird observations, which we'll split into 4 files for easy upload.
aves1 <- slice(aves, 1:100000)
aves2 <- slice(aves, 100001:200000)
aves3 <- slice(aves, 200001:300000)
aves4 <- slice(aves, 300001:600000)

# Split fish in half
fish1 <- slice(fish, 1:100000)
fish2 <- slice(fish, 100001:200000)
```

## Save the remaining .csv files

For the other groups, we just want to save everything to the correct folder. Make sure the date is changed in the second half of the code for each group.

```{r}
## **__THIS IS WHERE YOU WOULD CHANGE THE DATES FOR EACH GROUP__**
write.csv (amphibia, file = "Data/January2024/GBIFClean Data/GBIF_amphibia_clean_January2024.csv", na="", row.names=FALSE)
write.csv (mammalia, file = "Data/January2024/GBIFClean Data/GBIF_mammalia_clean_January2024.csv", na="", row.names=FALSE)
write.csv (reptilia, file = "Data/January2024/GBIFClean Data/GBIF_reptilia_clean_January2024.csv", na="", row.names=FALSE)
write.csv (aves1, file = "Data/January2024/GBIFClean Data/GBIF_aves1_clean_January2024.csv", na="", row.names=FALSE)
write.csv (aves2, file = "Data/January2024/GBIFClean Data/GBIF_aves2_clean_January2024.csv", na="", row.names=FALSE)
write.csv (aves3, file = "Data/January2024/GBIFClean Data/GBIF_aves3_clean_January2024.csv", na="", row.names=FALSE)
write.csv (aves4, file = "Data/January2024/GBIFClean Data/GBIF_aves4_clean_January2024.csv", na="", row.names=FALSE)
write.csv (fish1, file = "Data/January2024/GBIFClean Data/GBIF_fish1_clean_January2024.csv", na="", row.names=FALSE)
write.csv (fish2, file = "Data/January2024/GBIFClean Data/GBIF_fish2_clean_January2024.csv", na="", row.names=FALSE)
write.csv (inverts, file = "Data/January2024/GBIFClean Data/GBIF_invertsNONbugs_clean_January2024.csv", na="", row.names=FALSE)

```

Voila! You did it!

You'll need to compress any files over 100 kb (just make them a zip folder) before uploading to Cal-IBIS. Plants, Aves and Fish are the usual big bois to zip. 
