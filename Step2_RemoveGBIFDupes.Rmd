---
title: "Remove_Duplicate_Data"
author: "Josie Lesage"
date: "12/9/2020"
output: html_document
editor: "Kylie Etter"
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = TRUE)

# Setup packages
library(rgbif)
library(tidyverse)
library(stringr)
```

*KJE Step 2A. First you are going to need to download some of the non-GBIF occurrences you just uploaded to Cal-IBIS. We want to compare occurrences between collections (i.e. Bugs-Ecdysis and Bugs-GBIF), to avoid inflated numbers caused by duplication of the same occurrence. Download the Bugs (2), Fungi (1), Plants (2) and Inverts (1) collections and put them in your respective "Data/MonthYear/" folder with the following names:

Bugs-Ecdysis & Scan Portal : EcdysisSCAN_Clean.csv
Fungi - Mycology Portal: MyCoPortal_Clean.csv
Plants- Consortium of California Herbaria 2 & Plants - SEINet: CCH2SEINet_Clean.csv
Invertebrates - InvertEBase:  InvertEBase_Clean.csv

To download data from Cal-IBIS click "Search Collections", click on the collection(s) you want to download, click "Next" on the "Search Criteria" page because we want everything, change the DownloadType to "CSV/ZIP", and then finally click the download button. Keep everything as is and download the data as a zip file, rename the occurrences.csv as instructed above and place it in the folder (only the renamed occurences.csv should go in the folder).


------------------------------------------------------------------------
Step 2B. 

## Cleaning data and removing potential duplicates

The code chunks below will remove the duplicated occurrence records for all records gathered from GBIF that are already present in the datasets downloaded from other sources (fungi, arthros, plants). To do this, we will directly call on the other dataset data and remove all duplicates from the data we downloaded from GBIF above.

Below that, there's a chunk to just extract and save the .csv files to the clean data folder.

!!FIRST STEP: Use ctrl+F to find and *replace all* the previous date ("October2024") with the new date, the name of the MonthYear folder you just created.!!!

All zip files should be extracted before running the code below. 

### Arthropods

```{r removing duplicates from Bugs aka Arthropods}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU need the DATE changed with replace all instructed above__**
arthro_raw <- read.delim("Data/October2024/Raw/Arthropods/occurrence.txt") 

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# arthro_clean1 <- anti_join(arthro_raw, inCode, by = "institutionCode")
# arthro_clean2 <- anti_join(arthro_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
ecdysisscan_clean <- read_csv("Data/October2024/EcdysisSCAN_Clean.csv", 
                       col_types = cols(.default = "c")) 

# these steps clean the GBIF arthro data by filtering out anything that already has a match in Ecdysis or SCAN
arthro_process <- anti_join(arthro_raw, ecdysisscan_clean, by = "occurrenceID")


# This line saves the data as a .csv in the appropriate folder with a new name
write.csv (arthro_process, 
           file = "Data/October2024//GBIFClean Data/GBIF_arthropodsBUGS_clean_October2024.csv",
           na="", row.names=FALSE)


#removing to make some more space, if you need any of these files again you will have to redownload/clean
remove(arthro_raw, arthro_process, ecdysisscan_clean)
```

### Fungi

```{r removing duplicates from Fungi}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
fungi_raw <- read.delim("Data/October2024/Raw/Fungi/occurrence.txt")

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# fungi_clean1 <- anti_join(fungi_raw, inCode, by = "institutionCode")
# fungi_clean2 <- anti_join(fungi_clean1, inCode, by = "collectionCode")

## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
MyCo_clean <- read_csv("Data/October2024/MyCoPortal_Clean.csv", 
                       col_types = cols(.default = "c"))

# these steps clean the GBIF fungi data by filtering out anything that already has a match  in MyCoPortal
fungi_process <- anti_join(fungi_raw, MyCo_clean, by = "occurrenceID")


# This line saves the data as a .csv in the appropriate folder with a new name
write.csv (fungi_process, 
           file = "Data/October2024/GBIFClean Data/GBIF_fungi_clean_October2024.csv",
           na="", row.names=FALSE)

#removing to make some more space
remove(fungi_raw, fungi_process, MyCo_clean)
```

### Plants
```{r removing duplicates from Plants}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
plant_raw <- read_tsv("Data/October2024/Raw/Plants/occurrence.txt", col_types = cols(.default = "c"))



## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# plant_clean1 <- anti_join(plant_raw, inCode, by = "institutionCode")
# plant_clean2 <- anti_join(plant_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
cch2seinet_clean <- read_csv("Data/October2024/CCH2SEINet_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF plant data by filtering out anything that already has a match in CCH or SEINet.
plant_process <- anti_join(plant_raw, cch2seinet_clean, by = "occurrenceID")

# This line saves the data as a .csv in the appropriate folder with a new name
write.csv (plant_process, file = "Data/October2024/GBIFClean Data/GBIF_plants_clean_October2024.csv", na="", row.names=FALSE)

#removing to make some more space
remove(plant_raw, plant_process, cch2seinet_clean)
```

### Invertebrates
```{r removing duplicates from Invertebrates}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
invert_raw <- read_tsv("Data/October2024/Raw/Inverts/occurrence.txt", col_types = cols(.default = "c"))

## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
invertEBase_clean <- read_csv("Data/October2024/InvertEBase_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF plant data by filtering out anything that already has a match in CCH or SEINet.
invert_process <- anti_join(invert_raw, invertEBase_clean, by = "occurrenceID")

# This line saves the data as a .csv in the appropriate folder with a new name
write.csv (invert_process, file = "Data/October2024/GBIFClean Data/GBIF_inverts_clean_October2024.csv", na="", row.names=FALSE)

#removing to make some more space
remove(invert_raw, invert_process, invertEBase_clean)
```
## Cut up the big files into multiple parts

Converting the downloaded GBIF data into CSVs and cutting the large datasets (>150000 obs.) into smaller pieces makes it easier to upload to symbiota. We just want to save everything to the correct folder. Make sure the date is correct in the second half of the code for each group (which should have been changed before starting this document code).
**In the future, we may need to increase the number of slices.**
```{r Convert ones that don't need to be cut}
amphibia <- read.delim("Data/October2024/Raw/Amphibia/occurrence.txt", quote = "")
mammalia <- read.delim("Data/October2024/Raw/Mammals/occurrence.txt", quote = "")
reptilia <- read.delim("Data/October2024/Raw/Reptiles/occurrence.txt", quote = "")

## Creates csvs in the appropriate folder
write.csv (amphibia, file = "Data/October2024/GBIFClean Data/GBIF_amphibia_clean_October2024.csv", na="", row.names=FALSE)
write.csv (mammalia, file = "Data/October2024/GBIFClean Data/GBIF_mammalia_clean_October2024.csv", na="", row.names=FALSE)
write.csv (reptilia, file = "Data/October2024/GBIFClean Data/GBIF_reptilia_clean_October2024.csv", na="", row.names=FALSE)

#removing to make some more space
remove(amphibia, mammalia, reptilia)
```
```{r Cutting up big datasets}
aves <- read.delim("Data/October2024/Raw/Aves/occurrence.txt", quote = "")
fish <- read.delim("Data/October2024/Raw/Fish/occurrence.txt", quote = "")

# There are just over 500,000 bird observations, which we'll split into 5 files for easy upload.
aves1 <- slice(aves, 1:120000)
aves2 <- slice(aves, 120001:240000)
aves3 <- slice(aves, 240001:360000)
aves4 <- slice(aves, 360001:480000)
aves5 <- slice(aves, 480001:600000)

# Split fish in half
fish1 <- slice(fish, 1:100000)
fish2 <- slice(fish, 100001:200000)

## Creates csvs in the appropriate folder
write.csv (aves1, file = "Data/October2024/GBIFClean Data/GBIF_aves1_clean_October2024.csv", na="", row.names=FALSE)
write.csv (aves2, file = "Data/October2024/GBIFClean Data/GBIF_aves2_clean_October2024.csv", na="", row.names=FALSE)
write.csv (aves3, file = "Data/October2024/GBIFClean Data/GBIF_aves3_clean_October2024.csv", na="", row.names=FALSE)
write.csv (aves4, file = "Data/October2024/GBIFClean Data/GBIF_aves4_clean_October2024.csv", na="", row.names=FALSE)
write.csv (aves5, file = "Data/October2024/GBIFClean Data/GBIF_aves5_clean_October2024.csv", na="", row.names=FALSE)
write.csv (fish1, file = "Data/October2024/GBIFClean Data/GBIF_fish1_clean_October2024.csv", na="", row.names=FALSE)
write.csv (fish2, file = "Data/October2024/GBIFClean Data/GBIF_fish2_clean_October2024.csv", na="", row.names=FALSE)
```

Voila! You did it!

You'll need to compress any files over 100 kb (just make them a zip folder) before uploading to Cal-IBIS. Plants, Aves, Fish and non-insect Inverts are the usual suspects to zip. 
